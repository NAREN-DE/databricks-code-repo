{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba86a20-5a3a-4130-86f5-e312f4a7901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain Read & Write Ops Assignment - Building Datalake & Lakehouse\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b1d341-e6cf-4388-b3fd-8569d2d65bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS telecom_catalog_assign;\n",
    "CREATE SCHEMA IF NOT EXISTS telecom_catalog_assign.landing_zone;\n",
    "CREATE VOLUME IF NOT EXISTS telecom_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43582373-8921-436b-b0dd-82b8dc567e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol\"\n",
    "\n",
    "dbutils.fs.mkdirs(f\"{base_path}/customer\")\n",
    "dbutils.fs.mkdirs(f\"{base_path}/usage\")\n",
    "dbutils.fs.mkdirs(f\"{base_path}/tower\")\n",
    "\n",
    "print(\"Landing zone folders created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations\n",
    "1. Write dbutils.fs code to copy the above datasets into your created Volume folders:\n",
    "Customer ‚Üí /Volumes/.../customer/\n",
    "Usage ‚Üí /Volumes/.../usage/\n",
    "Tower (region-based) ‚Üí /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e474796c-0d0b-4de3-b91c-6d5cc45e8665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = \"\"\"101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "\"\"\"\n",
    "\n",
    "usage_tsv = \"\"\"customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "\"\"\"\n",
    "\n",
    "tower_logs_region1 = \"\"\"event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "\"\"\"\n",
    "tower_logs_region2 = \"\"\"event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "\"\"\"\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_region2.csv\", tower_logs_region1, overwrite=True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", customer_csv, overwrite=True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", usage_tsv, overwrite=True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_region1.csv\", tower_logs_region1, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Spark Directory Read Use Cases\n",
    "1. Read all tower logs using:\n",
    "Path glob filter (example: *.csv)\n",
    "Multiple paths input\n",
    "Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba414bcd-dc07-456a-8378-08e1f406e96f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_glob_df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.csv\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\")\n",
    "\n",
    "tower_glob_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8af4fe17-f87c-46c2-adca-08fa72e37466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled ‚Äúabc‚Äù in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56cd84d-1bb4-498a-b6dd-f63cbd8e8df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df_opt1 = spark.read \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "cust_df_opt1.printSchema()\n",
    "cust_df_opt1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28f5cb1-68bd-483b-9787-b27c994a5a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df_opt1 = spark.read \\\n",
    "    .options(\n",
    "        header=\"false\",\n",
    "        inferSchema=\"false\",\n",
    "        sep=\"\\t\"\n",
    "    ) \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "\n",
    "usage_df_opt1.printSchema()\n",
    "usage_df_opt1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf3f11f2-18f6-42ca-9f9d-01a18b77dc9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df_opt2 = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .options(\n",
    "        header= \"false\",\n",
    "        inferSchema=\"true\",\n",
    "        sep= \",\"\n",
    "    ) \\\n",
    "    .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "cust_df_opt2.printSchema()\n",
    "cust_df_opt2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26f89d6e-aad2-40da-96a2-d23f40876799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "| Setting          | What happens                                                                               |\n",
    "| ---------------- | ------------------------------------------------------------------------------------------ |\n",
    "| `header = false` | Spark treats the first row as **data** and assigns default column names (`_c0`, `_c1`, ‚Ä¶). |\n",
    "| `header = true`  | Spark treats the first row as **column names** and does not include it in the data.        |\n",
    "| `inferSchema = false` | Spark reads **all columns as STRING** without validating data types.             |\n",
    "| `inferSchema = true`  | Spark scans the data and **infers the most suitable data type** for each column. |\n",
    "Spark attempts to infer age as a numeric column.\n",
    "While scanning rows, Spark encounters the value \"abc\" (non-numeric).\n",
    "Because schema inference is pessimistic, Spark downgrades the entire column to STRING.\n",
    "No error is thrown and no value is converted to NULL automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95631d2b-42dd-4b8c-925d-e2a9965d9cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df_raw = spark.read.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\n",
    "    \"custid\",\n",
    "    \"fname\",\n",
    "    \"age\",\n",
    "    \"city\",\n",
    "    \"plan_type\"\n",
    ")\n",
    "cust_df_raw.printSchema()\n",
    "cust_df_raw.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73997fa9-f763-4431-a149-8cfc0c499b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_schema = \"customer_id INT, voice_mins INT, data_mb INT, sms_count INT\"\n",
    "\n",
    "usage_df = spark.read.option(\"header\", \"true\").option(\"sep\", \"\\t\").schema(usage_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "\n",
    "usage_df.printSchema()\n",
    "usage_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abdaf6a2-0782-4bf2-b137-76f23ba2e63e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, TimestampType\n",
    ")\n",
    "tower_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"tower_id\", StringType(), True),\n",
    "    StructField(\"signal_strength\", IntegerType(), True),\n",
    "    StructField(\"event_timestamp\", TimestampType(), True)\n",
    "])\n",
    "tower_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .schema(tower_schema) \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_region1.csv\")\n",
    "\n",
    "tower_df.printSchema()\n",
    "tower_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e1d6d88-7bcc-4548-a0d1-15d37f6fc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Write Operations using \n",
    "- csv, json, orc, parquet, delta, saveAsTable, insertInto, xml with different write mode, header and sep options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e34c3bc-962d-438d-a1b6-ac27d2da6608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Write Operations (Data Conversion/Schema migration) ‚Äì CSV Format Usecases\n",
    "1. Write customer data into CSV format using overwrite mode\n",
    "2. Write usage data into CSV format using append mode\n",
    "3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3046b000-ea7f-4815-a420-1cb7fd3e9882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df = spark.read \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "cust_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/customer_csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b46229-c600-4290-8a66-856cd55c8df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "usage_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/usage_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc1ba00-0bfb-47fb-bcfd-afc44f6eb0b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_region1.csv\")\n",
    "tower_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/tower_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c138e9-eaef-4e24-bd16-4a220750348e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tower_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7190c65-f717-43bf-9f89-ec018f67ccba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\n",
    "  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/customer_csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34158cf6-dd7f-40d6-9969-ed76710540a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Write Operations (Data Conversion/Schema migration)‚Äì JSON Format Usecases\n",
    "1. Write customer data into JSON format using overwrite mode\n",
    "2. Write usage data into JSON format using append mode and snappy compression format\n",
    "3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6605f67-237a-406d-b3a7-8fb0a3811ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/customer_json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8700232e-a834-4d1f-ae64-4ad335cc79a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/usage_json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec940d62-6771-4f7c-ab1b-041b8eb3f9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_df.write \\\n",
    "    .mode(\"ignore\") \\\n",
    "    .json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/tower_json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faca010d-8143-400f-b995-51d295f44377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_json_df = spark.read \\\n",
    "    .json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/tower_json\")\n",
    "\n",
    "tower_json_df.show(5)\n",
    "tower_json_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26f2ba69-3cde-4ec6-8945-e4ef9f7bb109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8. Write Operations (Data Conversion/Schema migration) ‚Äì Parquet Format Usecases\n",
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "2. Write usage data into Parquet format using error mode\n",
    "3. Write tower data into Parquet format with gzip compression option\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7619117d-f99d-4a76-a8f1-b43f47ad5849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/customer_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd26609-c8e7-419b-8110-48d5b939d088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df.write \\\n",
    "    .mode(\"error\") \\\n",
    "    .parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/usage_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd3fd1a9-fef5-49e7-91b6-dd4fea05fc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/tower_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506940d7-6fd2-4943-9541-f614b46edbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_parquet_df = spark.read \\\n",
    "    .parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/usage_parquet\")\n",
    "\n",
    "usage_parquet_df.show(5)\n",
    "usage_parquet_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41c794f-5cfc-4aeb-a599-e6d4a47a0f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##9. Write Operations (Data Conversion/Schema migration) ‚Äì Orc Format Usecases\n",
    "1. Write customer data into ORC format using overwrite mode\n",
    "2. Write usage data into ORC format using append mode\n",
    "3. Write tower data into ORC format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f1c926c-f40a-45da-81f1-578b944e2eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/customer_orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849d74d0-603a-47d7-8e3e-3a20c248c900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/usage_orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5253e3e-4045-4952-a155-6ddc4535bb1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/tower_orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dcbea7f-54ab-4bb5-b5bb-b0495c327805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_orc_df = spark.read \\\n",
    "    .orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/usage_orc\")\n",
    "\n",
    "usage_orc_df.show(5)\n",
    "usage_orc_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35761315-0b0f-46ff-9c3d-c0405bce7b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##10. Write Operations (Data Conversion/Schema migration) ‚Äì Delta Format Usecases\n",
    "1. Write customer data into Delta format using overwrite mode\n",
    "2. Write usage data into Delta format using append mode\n",
    "3. Write tower data into Delta format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16568d11-f354-4d21-9008-8807a407bed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/customer_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0545627d-f7a0-4746-ae6e-af738a0a7c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/usage_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bc1d972-ba0c-4d05-bb7a-303170018a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/tower_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38d280f-56e0-474d-be0b-44dc41d8343b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_delta_df = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/usage_delta\")\n",
    "\n",
    "usage_delta_df.show(5)\n",
    "usage_delta_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5633b581-d22d-4951-9de4-5606513baf90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Feature            | Parquet  | Delta                |\n",
    "| ------------------ | -------- | -------------------- |\n",
    "| File format        | Parquet  | Parquet              |\n",
    "| Transaction log    | ‚ùå No     | ‚úÖ Yes (`_delta_log`) |\n",
    "| ACID transactions  | ‚ùå No     | ‚úÖ Yes                |\n",
    "| Schema enforcement | ‚ùå No     | ‚úÖ Yes                |\n",
    "| Time travel        | ‚ùå No     | ‚úÖ Yes                |\n",
    "| Updates / deletes  | ‚ùå No     | ‚úÖ Yes                |\n",
    "| Concurrent writes  | ‚ùå Unsafe | ‚úÖ Safe               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6dd0890-02bd-4acd-b837-daceb256c706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##11. Write Operations (Lakehouse Usecases) ‚Äì Delta table Usecases\n",
    "1. Write customer data using saveAsTable() as a managed table\n",
    "2. Write usage data using saveAsTable() with overwrite mode\n",
    "3. Drop the managed table and verify data removal\n",
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "5. Use spark.read.sql to write some simple queries on the above tables created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6f4d89-36e6-4915-8b29-4da8aa4f1c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"telecom_catalog_assign.landing_zone.customer_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45119480-445f-49f9-9ac0-d423da946ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"telecom_catalog_assign.landing_zone.usage_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4ffdf5-9fa1-4243-9ba4-26ad8ead42d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE telecom_catalog_assign.landing_zone.customer_tbl;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49ea251-8c9b-4c1d-87b4-f7bd2584859b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES IN telecom_catalog_assign.landing_zone;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48f84b5-c5fb-4f73-944c-44b155817a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED telecom_catalog_assign.landing_zone.usage_tbl;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3adda25d-86f8-4008-9759-854f2c343ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM telecom_catalog_assign.landing_zone.usage_tbl\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aac447b-690b-4562-99dd-0ce096e9ad55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##12. Write Operations (Lakehouse Usecases) ‚Äì Delta table Usecases\n",
    "1. Write customer data using insertInto() in a new table and find the behavior\n",
    "2. Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71da2a5-4a51-411b-98e5-88ca2071e6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE telecom_catalog_assign.landing_zone.customer_ins_tbl (\n",
    "  custid INT,\n",
    "  fname STRING,\n",
    "  age STRING,\n",
    "  city STRING,\n",
    "  plan_type STRING\n",
    ")\n",
    "USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d179a300-fd47-45bf-9e9c-4ee66193fd83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df.write.insertInto(\n",
    "    \"telecom_catalog_assign.landing_zone.customer_ins_tbl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d119b225-528a-45f3-85ea-bcc107199ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE telecom_catalog_assign.landing_zone.usage_ins_tbl (\n",
    "  customer_id INT,\n",
    "  voice_mins INT,\n",
    "  data_mb INT,\n",
    "  sms_count INT\n",
    ")\n",
    "USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e3f7e95-cbe8-4cd3-8c07-82ad85aa98b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .insertInto(\"telecom_catalog_assign.landing_zone.usage_ins_tbl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c4bce3-4bd3-4db6-a074-02bb24c5f91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##13. Write Operations (Lakehouse Usecases) ‚Äì Delta table Usecases\n",
    "1. Write customer data into XML format using rowTag as cust\n",
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f3f6dc-603f-48b6-9598-4f7d17b040c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"rowTag\", \"cust\") \\\n",
    "    .xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/customer_xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd988569-e847-4deb-ad74-55e1e68270f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"rowTag\", \"usage\") \\\n",
    "    .xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/output/usage_xml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e2fe69-9352-4ec9-bf70-15d760c89aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##14. Compare all the downloaded files (csv, json, orc, parquet, delta and xml) \n",
    "1. Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d6e39ec-752d-4183-9656-2b6d7938922d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##15. Do a final exercise of defining one/two liner of... \n",
    "1. When to use/benifits csv\n",
    "2. When to use/benifits json\n",
    "3. When to use/benifit orc\n",
    "4. When to use/benifit parquet\n",
    "5. When to use/benifit delta\n",
    "6. When to use/benifit xml\n",
    "7. When to use/benifit delta tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1c5211f-44da-4111-ac34-517b8536472c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìÑ CSV ‚Äì When to use / Benefits\n",
    "\n",
    "Use CSV for simple, flat data exchange where human readability and wide tool compatibility are required.\n",
    "\n",
    "Benefits:\n",
    "Easy to create and read, universally supported, good for small to medium datasets and landing/raw layers.\n",
    "\n",
    "üßæ JSON ‚Äì When to use / Benefits\n",
    "\n",
    "Use JSON for semi-structured or hierarchical data, especially from APIs or event streams.\n",
    "\n",
    "Benefits:\n",
    "Supports nested structures, flexible schema, easy parsing across applications, ideal for API and streaming data.\n",
    "\n",
    "üß± ORC ‚Äì When to use / Benefits\n",
    "\n",
    "Use ORC for high-performance analytics in Hadoop-centric ecosystems.\n",
    "\n",
    "Benefits:\n",
    "Columnar storage, strong compression, fast scans and aggregations, schema stored with data.\n",
    "\n",
    "üì¶ Parquet ‚Äì When to use / Benefits\n",
    "\n",
    "Use Parquet for analytical workloads and data lakes in cloud environments.\n",
    "\n",
    "Benefits:\n",
    "Columnar format, excellent compression, fast query performance, widely supported across big-data tools.\n",
    "\n",
    "üî∫ Delta (Delta file format) ‚Äì When to use / Benefits\n",
    "\n",
    "Use Delta when you need reliability and ACID guarantees on top of Parquet files.\n",
    "\n",
    "Benefits:\n",
    "Transaction log, schema enforcement, time travel, safe concurrent reads/writes.\n",
    "\n",
    "üßæ XML ‚Äì When to use / Benefits\n",
    "\n",
    "Use XML for legacy systems or enterprise integrations that require strict schema and hierarchical data.\n",
    "\n",
    "Benefits:\n",
    "Strong structure, supports validation (XSD), human-readable, common in legacy and SOAP-based systems.\n",
    "\n",
    "üèóÔ∏è Delta Tables (Lakehouse) ‚Äì When to use / Benefits\n",
    "\n",
    "Use Delta tables for production Lakehouse architectures where data reliability, governance, and SQL analytics are required.\n",
    "\n",
    "Benefits:\n",
    "ACID transactions, schema evolution, time travel, scalable analytics, unified batch + streaming, full governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a18dd30-a6f8-4cbe-895b-4d8b4534cd07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CSV      ‚Üí simple exchange\n",
    "JSON     ‚Üí semi-structured / APIs\n",
    "ORC      ‚Üí Hadoop analytics\n",
    "Parquet  ‚Üí cloud analytics\n",
    "Delta    ‚Üí reliable Parquet\n",
    "XML      ‚Üí legacy systems\n",
    "DeltaTbl ‚Üí production Lakehouse\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6634956720516458,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read_write_usecases",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
